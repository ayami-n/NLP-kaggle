{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# This is for storing data file path\nimportdata = '' # '' meaning for define data type(String) \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        importdata = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-06T12:20:50.003678Z","iopub.execute_input":"2021-09-06T12:20:50.004050Z","iopub.status.idle":"2021-09-06T12:20:50.026033Z","shell.execute_reply.started":"2021-09-06T12:20:50.003968Z","shell.execute_reply":"2021-09-06T12:20:50.025171Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/japanese-whisky-review/japanese_whisky_review.csv\n/kaggle/input/positive-words/positive.txt\n/kaggle/input/negative-words/nega.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom collections import Counter, deque\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:20:50.027336Z","iopub.execute_input":"2021-09-06T12:20:50.027590Z","iopub.status.idle":"2021-09-06T12:21:31.508051Z","shell.execute_reply.started":"2021-09-06T12:20:50.027565Z","shell.execute_reply":"2021-09-06T12:21:31.506891Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading vader_lexicon: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(importdata)\ndata","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.509566Z","iopub.execute_input":"2021-09-06T12:21:31.509830Z","iopub.status.idle":"2021-09-06T12:21:31.548006Z","shell.execute_reply.started":"2021-09-06T12:21:31.509804Z","shell.execute_reply":"2021-09-06T12:21:31.547011Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         2-faced\n0        2-faces\n1       abnormal\n2        abolish\n3     abominable\n4     abominably\n...          ...\n4777        zaps\n4778      zealot\n4779     zealous\n4780   zealously\n4781       zombi\n\n[4782 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2-faced</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2-faces</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abnormal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abolish</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abominable</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abominably</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4777</th>\n      <td>zaps</td>\n    </tr>\n    <tr>\n      <th>4778</th>\n      <td>zealot</td>\n    </tr>\n    <tr>\n      <th>4779</th>\n      <td>zealous</td>\n    </tr>\n    <tr>\n      <th>4780</th>\n      <td>zealously</td>\n    </tr>\n    <tr>\n      <th>4781</th>\n      <td>zombi</td>\n    </tr>\n  </tbody>\n</table>\n<p>4782 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"print(data.info())  # find Nan, number of columns object(5)\nprint(data.describe())  # check the values\npd.set_option('display.max_columns', 6)  # display all columns (object +1 = 6)\nprint(data.head())\nBottle = data.groupby(['Bottle_name']).size()\nprint(Bottle)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.562884Z","iopub.execute_input":"2021-09-06T12:21:31.563169Z","iopub.status.idle":"2021-09-06T12:21:31.714477Z","shell.execute_reply.started":"2021-09-06T12:21:31.563142Z","shell.execute_reply":"2021-09-06T12:21:31.713085Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4782 entries, 0 to 4781\nData columns (total 1 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   2-faced  4782 non-null   object\ndtypes: object(1)\nmemory usage: 37.5+ KB\nNone\n             2-faced\ncount           4782\nunique          4782\ntop     dissapointed\nfreq               1\n      2-faced\n0     2-faces\n1    abnormal\n2     abolish\n3  abominable\n4  abominably\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-b7b4972dc583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# display all columns (object +1 = 6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mBottle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bottle_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6725\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6726\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6727\u001b[0;31m             \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6728\u001b[0m         )\n\u001b[1;32m   6729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             )\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Bottle_name'"],"ename":"KeyError","evalue":"'Bottle_name'","output_type":"error"}]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"data['Brand'].value_counts().plot(kind='barh')\nplt.show()\n\nplt.subplot(1, 2, 1)\n# opt = [\"Yamazaki\"] # this is for isin(), if I need to specify more, I cam write opt = ['Yamazaki', 'banrd name']\nx = data.loc[data['Brand'].isin([\"Yamazaki\"])] # extract all values (rows) which specified by using isin() function\nx['Bottle_name'].value_counts().plot(kind='bar')\nplt.title(\"Yamazaki and Bottle_Name\")\n\nplt.subplot(1, 2, 2)\ny = data.loc[data['Brand'].isin([\"Hibiki\"])]\ny['Bottle_name'].value_counts().plot(kind='bar', color='green')\nplt.title(\"Hibiki and Bottle_Name\")\nplt.show()\n\nplt.subplot(1, 2, 1)\nz = data.loc[data['Brand'].isin([\"Nikka\"])]\nz['Bottle_name'].value_counts().plot(kind='bar', color='orange')\nplt.title(\"Kikka and Bottle_Name\")\n\nplt.subplot(1, 2, 2)\nw = data.loc[data['Brand'].isin([\"Hakushu\"])]\nw['Bottle_name'].value_counts().plot(kind='bar', color='red')\nplt.title(\"Hakushu and Bottle_Name\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.715465Z","iopub.status.idle":"2021-09-06T12:21:31.715848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the Texts","metadata":{}},{"cell_type":"code","source":"corpus = [] # corpus will only get in the end all the clean reviews\nfor i in range(len(data)):\n    review = re.sub('[^a-zA-Z0-9_]', ' ', data['Review_Content'][i]) # for  remove all punctuation\n    review = review.lower() # all the capital letters were turned into lowercase\n    # review = review.split() # split different words\n    words = word_tokenize(review) # split words\n    ps = PorterStemmer() # loved -> love remove ed\n    stop = set(stopwords.words('english'))\n    stop.update(('Yamazaki', 'hibiki', 'it', 'whisky', 'whiskey', 'bottle')) # update stop words\n    review = [ps.stem(word) for word in words if not word in stop]\n    review = ' '.join(review) # adding space between each word of review\n    corpus.append(review)\n\ncount = dict(Counter(word for sentence in corpus for word in sentence.split()))\ndf = list(count.items())  # count.items() to return a collection of the key-value pairs in count\n# list(obj) with this collection as obj to convert in to a list\n\narray = np.array(df)  # np.array(df) with this list as data to convert it to array (make 2D)\nprint(array)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.716992Z","iopub.status.idle":"2021-09-06T12:21:31.717404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Positive, Negative, Neutral word list","metadata":{}},{"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\npos_word_list = []\nneu_word_list = []\nneg_word_list = []\n\nfor word in count:\n    if (sid.polarity_scores(word)['compound']) >= 0.5:\n        pos_word_list.append(word)\n    elif (sid.polarity_scores(word)['compound']) <= -0.5:\n        neg_word_list.append(word)\n    else:\n        neu_word_list.append(word)\n\nprint('Positive :', pos_word_list)\nprint('Neutral :', neu_word_list)\nprint('Negative :', neg_word_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.718279Z","iopub.status.idle":"2021-09-06T12:21:31.718652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP","metadata":{}},{"cell_type":"code","source":"def prev_and_next(input_list):\n    CURRENT = input_list\n    PREV = deque(input_list)\n    PREV.rotate(-1)\n    PREV = list(PREV)\n    NEXT = deque(input_list)\n    NEXT.rotate(1)\n    NEXT = list(NEXT)\n    return zip(PREV, CURRENT, NEXT)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.719391Z","iopub.status.idle":"2021-09-06T12:21:31.719765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding a column ('Sentiment') for NLP","metadata":{}},{"cell_type":"code","source":"data['Sentiment'] = 0\n\npositiveList = pd.read_csv('/kaggle/input/positive-words/positive.txt') # import all positive words from online\nnegativelist = pd.read_csv('/kaggle/input/negative-words/nega.txt')\n\npos = positiveList.iloc[:, 0].values # for lemmatizer use: converting all words to the single distinct list 'awesome','good'\nneg = negativelist.iloc[:, 0].values # converting all words to the single distinct list 'bad','hate'\n\nindex = 0 # accessing all rows from 1\n\nlemmatizer = WordNetLemmatizer() # modeling and lemmatization that convert plural to singular ex) Feet -> Foot\n\nps = PorterStemmer()  # loved -> love remove ed\nstop = set(stopwords.words('english'))\nstop.remove('not') # for checking Not + any positive words: Not Like\nstempos = [ps.stem(word) for word in pos if not word in stop] # for positiveList Ps use: this is for the review which will applied stemming in the loop\nstemnega = [ps.stem(word) for word in neg if not word in stop] # this is for the review which will applied stemming in the loop\n\ncorpus = [] # collecting all stemming and lemmatazed words\nfor i in data['Review_Content']:\n    review = re.sub('[^a-zA-Z]', ' ', i)  # for  remove all punctuation\n    review = review.lower()  # all the capital letters were turned into lowercase\n    words = word_tokenize(review) # split words\n\n    lemWords = [lemmatizer.lemmatize(w) for w in words] # for words lemmatizer\n\n    # This loop will stop when itr finds one of the positive or negative words (lemmatization) in a sentence.\n    for itr in lemWords: # check a word all in lemWords(h)\n        if itr in pos:   # check h in pos\n            data.at[index, 'Sentiment'] = 1 # found positive word first\n            break # finish loop\n        elif itr in neg: # check h in neg\n            data.at[index, 'Sentiment'] = 0 # found negative word first\n            break\n\n    review = [ps.stem(word) for word in lemWords if not word in stop] # for NLP by lemWords PorterStemmer\n\n    # I assumed that sentences that have any negative word first are already negative.\n    # This loop point is for finding 'Not + any positive words' such as \"NOT LIKE\"\n    if (data.at[index, 'Sentiment'] == 1): # for find next has negative word    'It's okay but I do not like it'\n        for previous, current, next in prev_and_next(review): # assume the next words are any positive words\n            if current == 'not':\n                if next in stempos: # stempos is stemmed as review needs to be stemmed for NLP\n                    data.at[index, 'Sentiment'] = 0\n                    break # no longer this loop needs to search\n\n    index += 1 # index is increasing\n    review = ' '.join(review)\n    corpus.append(review) # stores stemmed and lemmatized words here\n\nsentiment = data['Sentiment'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\nprint(sentiment)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.720593Z","iopub.status.idle":"2021-09-06T12:21:31.720954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis for NLP","metadata":{}},{"cell_type":"code","source":"count = dict(Counter(word for sentence in corpus for word in sentence.split()))\ndf = list(count.items())  # count.items() to return a collection of the key-value pairs in count\n# list(obj) with this collection as obj to convert in to a list\n\narray = np.array(df)  # np.array(df) with this list as data to convert it to array (make 2D)\narray1d = np.array(array[:, -1], dtype='i') # i = int\n\n# describe outlier\nprint(np.median(array1d))\nprint(np.mean(array1d))\nprint(np.std(array1d))  # for check the array type\nprint(array1d[(array1d>np.quantile(array1d, 0.1)) & (array1d<np.quantile(array1d, 0.9))].tolist())\n\n# visualizing outlier\nplt.boxplot(array1d)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.721939Z","iopub.status.idle":"2021-09-06T12:21:31.722341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Removing Words from the Above Steps","metadata":{}},{"cell_type":"code","source":"removewords = [word for word in count if (count[word] > 50) & (not word in stempos and not word in stemnega)]\n\nremoveIndex = []\nfor word in removewords:\n    if word in corpus:\n        removeIndex.append(corpus.index(word))\n        corpus.remove(word)\ndata = data.drop(removeIndex)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.723105Z","iopub.status.idle":"2021-09-06T12:21:31.723470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Bag of Words model","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer()\nx = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values\n\nmax_words = int(len(x[0] * 0.9))\ncv = CountVectorizer(max_features=max_words)\nx = cv.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.724292Z","iopub.status.idle":"2021-09-06T12:21:31.724658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying NLP","metadata":{}},{"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n\n# Training the Naive Bayes model on the Training set\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T12:21:31.725505Z","iopub.status.idle":"2021-09-06T12:21:31.725875Z"},"trusted":true},"execution_count":null,"outputs":[]}]}